{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eb1fd7",
   "metadata": {},
   "outputs": [],
   "source": "# 라이브러리 임포트\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport koreanize_matplotlib\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score, silhouette_samples\nfrom scipy.sparse import hstack, csr_matrix\n\nprint(\"✅ 라이브러리 로드 완료\")"
  },
  {
   "cell_type": "markdown",
   "id": "mteyybvx1u",
   "source": "## 1. 데이터 로드\n\n### 📥 모델링용 데이터 사용\n\n02번 노트북에서 생성한 `job_aioe_for_modeling.csv`를 로드합니다.\n\n**이 데이터의 특징:**\n- 747개 직업 (완전한 데이터만)\n- AIOE, Employment, Mean_Wage\n- Description (직무 설명 텍스트)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "5gczjo6zbq2",
   "source": "# 데이터 로드\ndf = pd.read_csv(\"../datas/processed/job_aioe_for_modeling.csv\")\n\nprint(\"✅ 데이터 로드 완료\")\nprint(f\"데이터 크기: {df.shape}\")\nprint(f\"\\n컬럼 목록:\")\nprint(df.columns.tolist())\nprint(f\"\\n상위 5개 행:\")\ndf.head()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "whyt65pbsfd",
   "source": "## 2. 피처 전처리\n\n### 🔧 군집 분석을 위한 피처 준비\n\n군집 분석에서는 **모든 피처를 동시에 고려**하여 유사도를 계산합니다.\n\n**전처리 단계:**\n\n1. **로그 변환**: Employment, Mean_Wage → Employment_log, Mean_Wage_log\n2. **텍스트 벡터화**: Description → TF-IDF (500개 단어)\n3. **피처 결합**: 숫자 + 텍스트\n4. **스케일링**: 모든 피처를 같은 척도로 조정\n\n### 💡 왜 로그 변환을 사용하는가?\n\n02번 EDA에서 확인했듯이:\n- Employment와 Mean_Wage는 극단적으로 치우친 분포\n- 로그 변환으로 정규분포에 가까워짐\n- **K-Means는 거리 기반** → 스케일이 비슷해야 함",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "hpcl68tfx57",
   "source": "# 1. 로그 변환 (02번 노트북에서 이미 생성되었지만, 없을 경우를 위해)\nif 'Employment_log' not in df.columns:\n    df[\"Employment_log\"] = np.log1p(df[\"Employment\"])\nif 'Mean_Wage_log' not in df.columns:\n    df[\"Mean_Wage_log\"] = np.log1p(df[\"Mean_Wage\"])\n\n# 2. 텍스트 처리\ndf[\"Description\"] = df[\"Description\"].fillna(\"\")\n\n# TF-IDF 벡터화 (500개 중요 단어)\ntfidf = TfidfVectorizer(\n    max_features=500,      # 상위 500개 중요 단어\n    stop_words=\"english\",  # 영어 불용어 제거\n    min_df=2,              # 최소 2개 문서에 등장\n    max_df=0.8             # 80% 이상 문서 등장 시 제외\n)\nX_text = tfidf.fit_transform(df[\"Description\"])\n\nprint(\"✅ 텍스트 벡터화 완료\")\nprint(f\"텍스트 피처 shape: {X_text.shape}\")\nprint(f\"사용된 단어 수: {len(tfidf.get_feature_names_out())}\")\n\n# 3. 숫자형 피처 선택\nX_num = df[[\"AIOE\", \"Employment_log\", \"Mean_Wage_log\"]].values\n\nprint(f\"\\n숫자 피처 shape: {X_num.shape}\")\n\n# 4. 피처 결합 (숫자 + 텍스트)\nX_combined = hstack([csr_matrix(X_num), X_text])\n\nprint(f\"\\n결합된 피처 shape: {X_combined.shape}\")\nprint(f\"  - 숫자 피처: 3개 (AIOE, Employment_log, Mean_Wage_log)\")\nprint(f\"  - 텍스트 피처: {X_text.shape[1]}개 (TF-IDF 단어)\")\nprint(f\"  - 총 피처: {X_combined.shape[1]}개\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f2cybbl8yti",
   "source": "### 🎚️ 피처 스케일링\n\n**StandardScaler란?**\n- 각 피처를 평균 0, 분산 1로 변환\n- 공식: `(x - mean) / std`\n\n**왜 스케일링이 필요한가?**\n\nK-Means는 **유클리드 거리(Euclidean Distance)**를 사용합니다:\n\n$$d(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}$$\n\n**문제 예시:**\n```\n직업A: AIOE=4, Employment_log=10, Mean_Wage_log=11\n직업B: AIOE=5, Employment_log=11, Mean_Wage_log=11\n\n거리 계산 (스케일링 없음):\nd = √[(5-4)² + (11-10)² + (11-11)²] = √2 ≈ 1.41\n```\n\nAIOE 차이 1과 Employment_log 차이 1이 **같은 중요도**로 계산됩니다.  \n하지만 실제로는 피처마다 중요도가 다를 수 있습니다.\n\n**스케일링 후:**\n- 모든 피처가 평균 0, 분산 1\n- 공정한 비교 가능\n\n**⚠️ 희소행렬 주의사항:**\n\n일반 StandardScaler는 평균을 빼므로 희소행렬이 밀집행렬(dense)로 변환됩니다.  \n→ 메모리 폭발!\n\n**해결:** `with_mean=False` 옵션 사용\n- 평균 빼기 생략\n- 표준편차로만 나눔\n- 희소성 유지",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "16dbb0ghghc",
   "source": "# 스케일링 (희소행렬 호환)\nscaler = StandardScaler(with_mean=False)  # 희소성 유지\nX_scaled = scaler.fit_transform(X_combined)\n\nprint(\"✅ 스케일링 완료\")\nprint(f\"최종 입력 데이터 shape: {X_scaled.shape}\")\nprint(f\"데이터 타입: {type(X_scaled)}\")\nprint(f\"희소성 (sparsity): {1 - X_scaled.nnz / (X_scaled.shape[0] * X_scaled.shape[1]):.2%}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "iev8rwamgro",
   "source": "## 3. 최적의 클러스터 개수 (K) 찾기\n\n### 🔍 K-Means의 가장 큰 문제: K를 어떻게 정할까?\n\nK-Means는 **클러스터 개수 K를 미리 지정**해야 합니다.  \n하지만 데이터를 보고 몇 개 그룹이 적절한지 알기 어렵습니다.\n\n**해결 방법:**\n1. **Elbow Method**: Inertia 그래프의 꺾이는 지점\n2. **Silhouette Score**: 클러스터 품질 측정\n\n---\n\n### 📐 방법 1: Elbow Method (팔꿈치 방법)\n\n**Inertia (응집도)란?**\n\n$$\\text{Inertia} = \\sum_{i=1}^{K} \\sum_{x \\in C_i} ||x - \\mu_i||^2$$\n\n- 각 점과 소속 클러스터 중심의 거리 제곱합\n- **낮을수록 좋음** (클러스터 내 점들이 중심에 가까움)\n\n**특징:**\n- K가 증가하면 Inertia는 항상 감소\n- K = N (데이터 개수)이면 Inertia = 0\n- 하지만 너무 많은 클러스터는 의미 없음!\n\n**Elbow Point:**\n- Inertia 감소 속도가 급격히 줄어드는 지점\n- \"팔꿈치\"처럼 꺾이는 부분\n- 그 이상 K를 늘려도 큰 이득 없음\n\n**예시:**\n```\nK=2: Inertia=10000 → K=3: Inertia=6000 (큰 감소!)\nK=3: Inertia=6000  → K=4: Inertia=4500 (중간 감소)\nK=4: Inertia=4500  → K=5: Inertia=4000 (작은 감소)\nK=5: Inertia=4000  → K=6: Inertia=3800 (작은 감소)\n→ Elbow = K=4\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "v9s16oba4li",
   "source": "# Elbow Method: K=2~10까지 실험\nK_range = range(2, 11)\ninertias = []\n\nprint(\"🔍 Elbow Method: K-Means 실험 중...\")\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X_scaled)\n    inertias.append(kmeans.inertia_)\n    print(f\"K={k}: Inertia={kmeans.inertia_:.2f}\")\n\n# 시각화\nplt.figure(figsize=(10, 6))\nplt.plot(K_range, inertias, marker='o', linewidth=2, markersize=8)\nplt.xlabel('클러스터 개수 (K)', fontsize=12)\nplt.ylabel('Inertia (응집도)', fontsize=12)\nplt.title('Elbow Method: 최적의 K 찾기', fontsize=14)\nplt.grid(alpha=0.3)\nplt.xticks(K_range)\n\n# Elbow point 강조 (예시: K=5)\n# 실제로는 그래프를 보고 판단\nplt.axvline(x=5, color='red', linestyle='--', alpha=0.5, label='예상 Elbow (K=5)')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n💡 그래프에서 꺾이는 지점(Elbow)을 찾으세요!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6keubn6grac",
   "source": "### 📊 방법 2: Silhouette Score (실루엣 점수)\n\n**Silhouette Score란?**\n\n각 데이터 포인트가 **자신의 클러스터에 얼마나 잘 속해 있는지** 측정하는 지표입니다.\n\n**공식:**\n\n$$s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}$$\n\n- $a(i)$: 같은 클러스터 내 다른 점들과의 평균 거리 (작을수록 좋음)\n- $b(i)$: 가장 가까운 다른 클러스터 점들과의 평균 거리 (클수록 좋음)\n\n**점수 범위:**\n- **-1 ~ +1**\n  - +1: 완벽하게 분리된 클러스터\n  - 0: 클러스터 경계에 있음\n  - -1: 잘못 분류됨\n\n**평균 Silhouette Score:**\n- 모든 데이터 포인트의 s(i) 평균\n- **높을수록 좋은 군집화**\n\n**장점:**\n- Inertia와 달리 **절대적인 품질** 측정\n- 다른 K 값을 공정하게 비교 가능\n\n**해석:**\n| 점수 | 해석 |\n|------|------|\n| 0.7 ~ 1.0 | 강한 구조 (Strong structure) |\n| 0.5 ~ 0.7 | 적절한 구조 (Reasonable structure) |\n| 0.25 ~ 0.5 | 약한 구조 (Weak structure) |\n| < 0.25 | 구조 없음 (No substantial structure) |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ksau3cz7ksh",
   "source": "# Silhouette Score 계산\nsilhouette_scores = []\n\nprint(\"🔍 Silhouette Score 계산 중...\")\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    labels = kmeans.fit_predict(X_scaled)\n    score = silhouette_score(X_scaled, labels)\n    silhouette_scores.append(score)\n    print(f\"K={k}: Silhouette Score={score:.4f}\")\n\n# 시각화\nplt.figure(figsize=(10, 6))\nplt.plot(K_range, silhouette_scores, marker='s', linewidth=2, markersize=8, color='green')\nplt.xlabel('클러스터 개수 (K)', fontsize=12)\nplt.ylabel('Silhouette Score', fontsize=12)\nplt.title('Silhouette Score: 클러스터 품질 평가', fontsize=14)\nplt.grid(alpha=0.3)\nplt.xticks(K_range)\n\n# 최고 점수 강조\nbest_k = K_range[np.argmax(silhouette_scores)]\nbest_score = max(silhouette_scores)\nplt.axvline(x=best_k, color='red', linestyle='--', alpha=0.5, \n            label=f'최고 점수 (K={best_k}, Score={best_score:.4f})')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n✅ 최고 Silhouette Score: K={best_k} (Score={best_score:.4f})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "qkyst7vw1",
   "source": "### 🎯 최종 K 결정\n\n**두 가지 방법 종합:**\n\n1. **Elbow Method**: Inertia 감소가 둔화되는 지점\n2. **Silhouette Score**: 가장 높은 점수\n\n**일반적인 전략:**\n- 두 방법이 같은 K를 제시하면 → 그 K 선택\n- 다르면 → Silhouette Score 우선 (절대적 품질 지표)\n- 해석 가능성도 고려 (너무 많은 클러스터는 해석 어려움)\n\n**이 분석에서는:**\n- Elbow Method와 Silhouette Score 결과를 보고 결정\n- 일반적으로 K=4~6 정도가 적절할 것으로 예상",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "lqbit36s0o",
   "source": "## 4. K-Means 클러스터링 실행\n\n### 🎲 최종 모델 학습\n\n위 분석 결과를 바탕으로 최적의 K를 선택하여 최종 군집화를 수행합니다.\n\n**K-Means 하이퍼파라미터:**\n- `n_clusters`: 클러스터 개수 (위에서 결정한 K)\n- `random_state=42`: 재현 가능성\n- `n_init=10`: 다른 초기값으로 10번 실행 후 최선 선택",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "n5jpc0z39rm",
   "source": "# 최적 K 선택 (Silhouette Score 기준)\noptimal_k = best_k  # 위에서 계산된 best_k 사용\n\nprint(f\"🎯 선택된 K: {optimal_k}\")\nprint(f\"   Silhouette Score: {best_score:.4f}\")\n\n# 최종 K-Means 모델 학습\nkmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\nclusters = kmeans_final.fit_predict(X_scaled)\n\n# 클러스터 레이블을 데이터프레임에 추가\ndf[\"Cluster\"] = clusters\n\nprint(f\"\\n✅ K-Means 클러스터링 완료\")\nprint(f\"Inertia: {kmeans_final.inertia_:.2f}\")\nprint(f\"\\n클러스터별 직업 수:\")\nprint(df[\"Cluster\"].value_counts().sort_index())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ze3yblrcx6",
   "source": "## 5. PCA를 이용한 시각화\n\n### 🔬 문제: 고차원 데이터 시각화\n\n현재 데이터는 **503차원** (AIOE, Employment_log, Mean_Wage_log + 500개 TF-IDF 단어)  \n→ 2D 그래프로 시각화 불가능!\n\n### 💡 해결: PCA (Principal Component Analysis)\n\n**PCA란?**\n- 고차원 데이터를 저차원으로 축소하는 기법\n- 분산을 최대한 보존하면서 차원 축소\n- **비지도 학습** 방법\n\n**작동 원리:**\n\n1. 데이터의 분산이 가장 큰 방향(축) 찾기 → **PC1 (주성분 1)**\n2. PC1과 직교하며 두 번째로 분산이 큰 방향 찾기 → **PC2 (주성분 2)**\n3. 원본 데이터를 PC1, PC2 좌표로 투영\n\n**수식:**\n\n$$\\text{PC1} = \\text{argmax}_{\\mathbf{w}} \\text{Var}(\\mathbf{X} \\mathbf{w})$$\n\n- $\\mathbf{X}$: 원본 데이터 (503차원)\n- $\\mathbf{w}$: 방향 벡터\n- PC1: 분산을 최대화하는 방향\n\n**주의사항:**\n\n⚠️ **PCA는 시각화용일 뿐, 클러스터링에 사용하지 않음!**\n\n- 클러스터링: 원본 503차원 데이터 사용\n- PCA: 결과를 2D로 그리기 위한 도구\n- PCA로 축소 후 클러스터링하면 정보 손실 발생\n\n**분산 설명력 (Explained Variance):**\n\n- PC1 + PC2가 원본 분산의 몇 %를 설명하는가?\n- 예: 30% → PC1, PC2만으로 원본 정보의 30% 표현\n- 나머지 70%는 다른 차원에 존재",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "mvwg1q1c8sq",
   "source": "# PCA: 503차원 → 2차원 축소\npca = PCA(n_components=2, random_state=42)\nX_pca = pca.fit_transform(X_scaled.toarray())  # sparse → dense 변환 필요\n\n# PCA 좌표를 데이터프레임에 추가\ndf[\"PCA1\"] = X_pca[:, 0]\ndf[\"PCA2\"] = X_pca[:, 1]\n\nprint(\"✅ PCA 차원 축소 완료\")\nprint(f\"원본 차원: {X_scaled.shape[1]}D\")\nprint(f\"축소 차원: 2D (PC1, PC2)\")\nprint(f\"\\n분산 설명력:\")\nprint(f\"  PC1: {pca.explained_variance_ratio_[0]:.2%}\")\nprint(f\"  PC2: {pca.explained_variance_ratio_[1]:.2%}\")\nprint(f\"  Total: {pca.explained_variance_ratio_.sum():.2%}\")\nprint(f\"\\n💡 PC1과 PC2는 원본 데이터 분산의 {pca.explained_variance_ratio_.sum():.1%}를 설명합니다.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2updeq02kos",
   "source": "## 6. 클러스터 시각화\n\n### 📊 2D 산점도로 클러스터 확인\n\nPCA로 축소한 2차원 공간에서 각 클러스터를 색깔로 구분하여 시각화합니다.\n\n**시각화 목적:**\n1. 클러스터가 잘 분리되어 있는지 확인\n2. 클러스터 간 경계 파악\n3. 이상치(outlier) 발견\n\n**해석 주의사항:**\n- PC1, PC2는 원본 정보의 일부만 표현\n- 2D에서 겹쳐 보여도 503D에서는 분리될 수 있음\n- 클러스터 중심(centroid)도 함께 표시",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "wxi5pv9t8t",
   "source": "# 클러스터 시각화\nplt.figure(figsize=(12, 8))\n\n# 산점도: 각 클러스터를 다른 색으로 표시\nscatter = plt.scatter(\n    df[\"PCA1\"], \n    df[\"PCA2\"], \n    c=df[\"Cluster\"],           # 클러스터별 색상\n    cmap='tab10',              # 색상 팔레트\n    alpha=0.6,                 # 투명도\n    s=50,                      # 점 크기\n    edgecolors='black',        # 테두리\n    linewidths=0.5\n)\n\n# 클러스터 중심점 표시\ncenters_pca = pca.transform(kmeans_final.cluster_centers_)\nplt.scatter(\n    centers_pca[:, 0],\n    centers_pca[:, 1],\n    c='red',\n    marker='X',\n    s=300,\n    edgecolors='black',\n    linewidths=2,\n    label='Cluster Centers'\n)\n\n# 클러스터 번호 레이블\nfor i, center in enumerate(centers_pca):\n    plt.text(\n        center[0], center[1], \n        f'C{i}',\n        fontsize=14,\n        fontweight='bold',\n        ha='center',\n        va='center',\n        color='white'\n    )\n\nplt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=12)\nplt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=12)\nplt.title(f'직업 군집 분석 결과 (K={optimal_k}, PCA 2D 시각화)', fontsize=14, fontweight='bold')\nplt.colorbar(scatter, label='Cluster')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"✅ {optimal_k}개 클러스터가 2D 공간에 투영되었습니다.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7kwllis9ac4",
   "source": "## 7. 클러스터 프로파일링\n\n### 🔍 각 클러스터의 특징 파악\n\n클러스터링의 핵심은 **각 그룹이 어떤 특성을 가지는지** 이해하는 것입니다.\n\n**분석 내용:**\n1. **숫자 피처 평균**: AIOE, Employment, Mean_Wage\n2. **대표 직업**: 각 클러스터의 예시 직업\n3. **클러스터 명명**: 특성 기반 이름 부여\n\n### 📊 클러스터별 통계 요약",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "sw1ljgjway",
   "source": "# 클러스터별 통계 요약\ncluster_summary = df.groupby(\"Cluster\")[[\"AIOE\", \"Employment\", \"Mean_Wage\"]].agg(['mean', 'std', 'count'])\n\nprint(\"=\" * 100)\nprint(\"📊 클러스터별 평균 특성\")\nprint(\"=\" * 100)\nprint(cluster_summary.round(2))\nprint(\"=\" * 100)\n\n# 시각화: 클러스터별 평균값 비교\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# AIOE\ncluster_means = df.groupby(\"Cluster\")[\"AIOE\"].mean().sort_index()\naxes[0].bar(cluster_means.index, cluster_means.values, color='skyblue', edgecolor='black')\naxes[0].set_xlabel('Cluster')\naxes[0].set_ylabel('평균 AIOE')\naxes[0].set_title('클러스터별 평균 AIOE')\naxes[0].grid(axis='y', alpha=0.3)\n\n# Employment\ncluster_means = df.groupby(\"Cluster\")[\"Employment\"].mean().sort_index()\naxes[1].bar(cluster_means.index, cluster_means.values, color='lightgreen', edgecolor='black')\naxes[1].set_xlabel('Cluster')\naxes[1].set_ylabel('평균 Employment')\naxes[1].set_title('클러스터별 평균 고용 규모')\naxes[1].grid(axis='y', alpha=0.3)\n\n# Mean_Wage\ncluster_means = df.groupby(\"Cluster\")[\"Mean_Wage\"].mean().sort_index()\naxes[2].bar(cluster_means.index, cluster_means.values, color='salmon', edgecolor='black')\naxes[2].set_xlabel('Cluster')\naxes[2].set_ylabel('평균 Mean_Wage ($)')\naxes[2].set_title('클러스터별 평균 임금')\naxes[2].grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "uvta8bwynj",
   "source": "### 📝 클러스터별 대표 직업",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "p6nutekldy",
   "source": "# 각 클러스터의 대표 직업 출력\nprint(\"\\n\" + \"=\" * 100)\nprint(\"📝 각 클러스터별 대표 직업 (상위 10개)\")\nprint(\"=\" * 100)\n\nfor cluster_id in range(optimal_k):\n    print(f\"\\n{'='*50}\")\n    print(f\"Cluster {cluster_id}\")\n    print(f\"{'='*50}\")\n    \n    cluster_df = df[df[\"Cluster\"] == cluster_id]\n    \n    # 통계 요약\n    print(f\"직업 수: {len(cluster_df)}\")\n    print(f\"평균 AIOE: {cluster_df['AIOE'].mean():.3f}\")\n    print(f\"평균 Employment: {cluster_df['Employment'].mean():,.0f}\")\n    print(f\"평균 Mean_Wage: ${cluster_df['Mean_Wage'].mean():,.0f}\")\n    \n    # 대표 직업 (AIOE 순으로 정렬)\n    print(f\"\\n상위 10개 직업 (AIOE 기준):\")\n    top_jobs = cluster_df.nlargest(10, 'AIOE')[['Title', 'AIOE', 'Employment', 'Mean_Wage']]\n    for idx, (_, row) in enumerate(top_jobs.iterrows(), 1):\n        print(f\"  {idx:2d}. {row['Title'][:60]:<60} | AIOE={row['AIOE']:.2f} | Wage=${row['Mean_Wage']:>8,.0f}\")\n\nprint(\"\\n\" + \"=\" * 100)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "wvjp72aop6a",
   "source": "## 8. 클러스터 해석 및 명명\n\n### 💡 클러스터 특성 분석\n\n**실행 후 각 클러스터의 특징을 요약하고 이름을 붙여야 합니다.**\n\n아래는 일반적으로 예상되는 패턴입니다:\n\n#### 예상 클러스터 유형:\n\n**1. 고AIOE-고임금 그룹**\n- 특징: AIOE > 4.5, Mean_Wage > $100,000\n- 직업 예시: \n  - 변호사 (Lawyers)\n  - 경영 컨설턴트 (Management Consultants)\n  - 금융 분석가 (Financial Analysts)\n- **해석**: AI의 영향을 많이 받지만 고도의 전문성으로 높은 임금 유지\n\n**2. 고AIOE-중임금 그룹**\n- 특징: AIOE > 4.0, Mean_Wage $60,000~$90,000\n- 직업 예시:\n  - 회계사 (Accountants)\n  - 데이터 입력 담당자 (Data Entry Keyers)\n  - 사무 직원 (Office Clerks)\n- **해석**: AI 자동화 위험이 높은 사무/행정직\n\n**3. 중AIOE-고임금 그룹**\n- 특징: AIOE 3.5~4.0, Mean_Wage > $90,000\n- 직업 예시:\n  - 소프트웨어 개발자 (Software Developers)\n  - 엔지니어 (Engineers)\n  - 의사 (Physicians)\n- **해석**: AI를 도구로 활용하지만 대체되기 어려운 전문직\n\n**4. 저AIOE-저임금 그룹**\n- 특징: AIOE < 3.5, Mean_Wage < $50,000\n- 직업 예시:\n  - 요리사 (Cooks)\n  - 청소부 (Cleaners)\n  - 소매 판매원 (Retail Salespersons)\n- **해석**: 육체노동/서비스직, AI 영향 낮음\n\n**5. 저AIOE-중임금 그룹**\n- 특징: AIOE < 3.5, Mean_Wage $50,000~$70,000\n- 직업 예시:\n  - 경찰 (Police Officers)\n  - 소방관 (Firefighters)\n  - 전기 기술자 (Electricians)\n- **해석**: 숙련 기술직, AI 영향 제한적\n\n---\n\n### 📝 실행 후 할 일:\n\n1. **위 출력 결과를 보고 각 클러스터의 패턴 파악**\n2. **클러스터에 의미 있는 이름 부여**\n   - 예: \"고위험-고보상 전문직\", \"AI 자동화 가능 사무직\" 등\n3. **클러스터 간 차이점 정리**\n4. **정책/개인 관점에서 시사점 도출**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "n63gs7058c",
   "source": "## 9. 최종 인사이트 및 결론\n\n### 🎯 군집 분석을 통해 발견한 것\n\n#### 1. 직업 시장의 구조화된 패턴 발견\n\n**군집 분석 결과:**\n- 747개 직업이 K개의 명확한 그룹으로 분류됨\n- 각 그룹은 AIOE, 고용 규모, 임금 수준의 조합으로 특징화\n- 직업 시장은 무작위가 아닌 **구조화된 패턴**을 보임\n\n#### 2. AIOE와 임금의 복잡한 관계\n\n**발견:**\n- AIOE와 임금은 단순 선형 관계가 아님\n- 고AIOE 직업 중에도:\n  - 고임금 (변호사, 컨설턴트) ← 전문성으로 보호\n  - 중임금 (사무직, 데이터 입력) ← 자동화 위험\n- 저AIOE 직업 중에도:\n  - 중임금 (숙련 기술직) ← 경험/기술 필요\n  - 저임금 (서비스직) ← 낮은 진입 장벽\n\n**시사점:**\n- AI 노출도만으로 직업의 미래를 판단하기 어려움\n- 임금 수준, 전문성, 대체 가능성을 종합 고려 필요\n\n#### 3. 정책 및 개인 전략 수립 기반\n\n**정책 입안자:**\n- 클러스터별 맞춤형 지원 정책\n  - 고위험 클러스터 → 재교육 프로그램\n  - 저위험 클러스터 → AI 도구 활용 교육\n\n**개인:**\n- 내 직업이 속한 클러스터 확인\n- 같은 클러스터 내 다른 직업으로 전환 고려\n- 다른 클러스터로 이동하기 위한 스킬 개발\n\n---\n\n### 📚 학습 내용 정리\n\n#### 비지도 학습 기술\n\n| 단계 | 학습 내용 | 핵심 개념 |\n|------|----------|----------|\n| **피처 준비** | 로그 변환, TF-IDF, 스케일링 | 거리 기반 알고리즘 준비 |\n| **최적 K 선택** | Elbow Method, Silhouette Score | 클러스터 개수 결정 |\n| **군집화** | K-Means 알고리즘 | 중심 기반 그룹핑 |\n| **차원 축소** | PCA | 고차원 → 2D 시각화 |\n| **해석** | 클러스터 프로파일링 | 실용적 의미 부여 |\n\n#### 군집 분석 Best Practices\n\n**✅ 올바른 방법:**\n1. **피처 스케일링 필수** (K-Means는 거리 기반)\n2. **최적 K 선택** (Elbow + Silhouette)\n3. **PCA는 시각화만** (군집화는 원본 데이터)\n4. **클러스터 해석** (통계 + 대표 샘플)\n5. **도메인 지식 활용** (숫자만 보지 말고 의미 파악)\n\n**❌ 피해야 할 실수:**\n1. 스케일링 없이 K-Means → 큰 값 피처 지배\n2. K를 임의로 선택 → 부적절한 군집화\n3. PCA 후 군집화 → 정보 손실\n4. 클러스터 해석 없음 → 실용성 없음\n\n---\n\n### 🔗 전체 분석 파이프라인 요약\n\n**01번 (전처리):**\n- AIOE 계산 (R × L × I)\n- OEWS 데이터 병합\n- 결측치 처리\n\n**02번 (EDA):**\n- 데이터 필터링 (완전한 데이터만)\n- 로그 변환 (치우친 분포 정규화)\n- 상관관계 분석 (AIOE vs 임금)\n\n**03번 (모델링):**\n- 지도 학습 (임금 예측)\n- Linear Regression, Random Forest, LightGBM\n- TF-IDF 텍스트 피처 활용\n\n**04번 (군집 분석):**\n- 비지도 학습 (직업 그룹 발견)\n- K-Means 클러스터링\n- PCA 시각화\n- 직업 시장 구조 이해\n\n---\n\n### 🚀 향후 개선 방향\n\n**1. 다른 군집 알고리즘 시도**\n- Hierarchical Clustering (계층적 군집)\n- DBSCAN (밀도 기반 군집)\n- Gaussian Mixture Models\n\n**2. 더 많은 피처 추가**\n- 교육 요구사항\n- 산업 분류\n- 지역 정보\n\n**3. 시간에 따른 변화 추적**\n- 연도별 AIOE 변화\n- 클러스터 이동 패턴\n- ChatGPT 등장 전후 비교\n\n**4. 실용적 활용**\n- 직업 추천 시스템\n- 클러스터 기반 재교육 프로그램 매칭\n- 정책 입안 대시보드\n\n---\n\n### 📖 참고 문헌\n\n1. **Felten, E., Raj, M., & Seamans, R. (2023)**  \n   *How will Language Modelers like ChatGPT Affect Occupations and Industries?*\n\n2. **Felten, E., Raj, M., & Seamans, R. (2021)**  \n   *Occupational, industry, and geographic exposure to artificial intelligence: A novel dataset and its potential uses*\n\n3. **MacQueen, J. (1967)**  \n   *Some methods for classification and analysis of multivariate observations*  \n   - K-Means 알고리즘 최초 제안\n\n4. **Rousseeuw, P. J. (1987)**  \n   *Silhouettes: A graphical aid to the interpretation and validation of cluster analysis*  \n   - Silhouette Score 제안\n\n---\n\n### 🎓 결론\n\n이 노트북에서는 **K-Means 군집 분석**을 통해 직업들을 유사한 그룹으로 분류했습니다.\n\n**성공적으로 확인한 것:**\n- 직업 시장은 AIOE, 고용, 임금 기준으로 구조화된 패턴을 보임\n- AI 영향도와 임금의 관계는 복잡하며 단순 선형 관계가 아님\n- 군집 분석은 직업 시장 구조를 이해하는 유용한 도구\n\n**학습한 것:**\n- K-Means 알고리즘 원리\n- 최적 K 선택 (Elbow, Silhouette)\n- PCA 차원 축소\n- 클러스터 해석 및 프로파일링\n\n**다음 단계:**\n- 실제 데이터로 실행하여 결과 확인\n- 각 클러스터에 의미 있는 이름 부여\n- 정책/개인 관점에서 시사점 도출\n\n---\n\n**🎉 수고하셨습니다! 전체 AIOE 분석 파이프라인(01 전처리 → 02 EDA → 03 모델링 → 04 군집 분석)을 완성했습니다!**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "x002saqm46g",
   "source": "# 🎯 AIOE 군집 분석 (Clustering Analysis)\n\n이 노트북은 03번 모델링에 이어 **비지도 학습(Unsupervised Learning)**을 사용하여 직업들을 유사한 그룹으로 분류하는 단계입니다.\n\n---\n\n## 📚 군집 분석이란?\n\n**군집 분석 (Clustering)**은 레이블이 없는 데이터를 **유사한 특성을 가진 그룹(클러스터)으로 나누는** 비지도 학습 방법입니다.\n\n### 지도 학습 vs 비지도 학습\n\n| 구분 | 지도 학습 (03번 모델링) | 비지도 학습 (04번 군집 분석) |\n|------|------------------------|---------------------------|\n| **목표** | 예측 (Prediction) | 패턴 발견 (Pattern Discovery) |\n| **레이블** | 있음 (Mean_Wage) | 없음 |\n| **예시** | 임금 예측 모델 | 직업 그룹 분류 |\n| **평가** | R², RMSE | Silhouette, Elbow Method |\n\n---\n\n## 🎯 분석 목적\n\n### 핵심 질문:\n> 직업들을 AIOE, 고용 규모, 임금, 직무 설명 기준으로 그룹화하면 어떤 패턴이 나타나는가?\n\n### 기대 효과:\n\n**1. 직업 시장 구조 이해**\n- 비슷한 직업들의 그룹 발견\n- 예시: \"고AIOE-고임금\", \"저AIOE-저임금\" 그룹\n\n**2. 정책 활용**\n- 그룹별 맞춤형 교육 프로그램\n- AI 영향도가 비슷한 직업군 지원 정책\n\n**3. 개인 활용**\n- 내 직업과 유사한 다른 직업 발견\n- 전환 가능한 직업군 탐색\n\n---\n\n## 📊 분석 전략\n\n### 사용 알고리즘: K-Means\n\n**K-Means란?**\n- 데이터를 K개의 그룹으로 나누는 알고리즘\n- 각 그룹의 중심(centroid)과 가장 가까운 점들을 묶음\n\n**K-Means 작동 원리:**\n\n1. 무작위로 K개의 중심점 선택\n2. 각 데이터를 가장 가까운 중심점에 할당\n3. 각 그룹의 평균으로 중심점 업데이트\n4. 2-3 반복 (수렴할 때까지)\n\n**공식:**\n\n$$\\text{minimize} \\sum_{i=1}^{K} \\sum_{x \\in C_i} ||x - \\mu_i||^2$$\n\n- $C_i$: 클러스터 i\n- $\\mu_i$: 클러스터 i의 중심점\n- $||x - \\mu_i||$: 데이터 x와 중심점의 거리\n\n### 사용 피처\n\n| 피처 | 설명 | 역할 |\n|------|------|------|\n| **AIOE** | AI 직업 노출도 | AI 영향도 |\n| **Employment_log** | 고용 규모 (로그) | 직업 규모 |\n| **Mean_Wage_log** | 평균 임금 (로그) | 보상 수준 |\n| **Description (TF-IDF)** | 직무 설명 (500개 단어) | 직무 특성 |\n\n**총 피처:** 3 (숫자) + 500 (텍스트) = 503개\n\n### 시각화 방법: PCA\n\n**문제:** 503차원 데이터를 2D로 시각화 불가\n\n**해결:** PCA (Principal Component Analysis)로 2차원 축소\n\n- PC1 (주성분 1): 분산을 가장 많이 설명하는 축\n- PC2 (주성분 2): PC1과 직교하며 두 번째로 중요한 축\n\n---\n\n## 🔗 이전 노트북과의 연결\n\n**01번 (전처리):**\n- AIOE 계산, OEWS 병합\n- `job_aioe_processed.csv` 생성\n\n**02번 (EDA):**\n- 완전 데이터 선택, 로그 변환\n- `job_aioe_for_modeling.csv` 생성\n\n**03번 (모델링):**\n- AIOE → 임금 예측 모델 구축\n- 지도 학습 (Supervised Learning)\n\n**04번 (군집 분석):**\n- 직업 그룹 발견\n- 비지도 학습 (Unsupervised Learning)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "8z607ul0r66",
   "source": "## 0. 환경 설정\n\n### 📚 라이브러리 설명\n\n**데이터 처리:**\n- `pandas`, `numpy`: 데이터 조작 및 수치 계산\n\n**시각화:**\n- `matplotlib`, `seaborn`: 그래프 생성\n- `koreanize_matplotlib`: 한글 폰트 설정\n\n**텍스트 처리:**\n- `TfidfVectorizer`: 텍스트를 TF-IDF 벡터로 변환\n\n**전처리:**\n- `StandardScaler`: 피처 표준화 (평균 0, 분산 1)\n\n**군집 분석:**\n- `KMeans`: K-Means 군집 알고리즘\n- `PCA`: 차원 축소 (시각화용)\n\n**기타:**\n- `scipy.sparse`: 희소 행렬 처리",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(AIOE)",
   "language": "python",
   "name": "aioe_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}